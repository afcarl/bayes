{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model assessment: LOO-CV for factory data with Stan (6p)\n",
    "\n",
    "Use leave-one-out cross validation (LOO-CV) to assess the predictive performance of the pooled, separate and hierarchical Gaussian models for the factory dataset (see the second exercise in Assignment 5). Use Stan for fitting the models, and the provided PSIS-LOO (Pareto smoothed importance sampling LOO) code for computing the approximate LOO-CV given the posterior samples provided by Stan. Your results should include:\n",
    "\n",
    " * PSIS-LOO values, the effective number of parameters peff, and the k-values for each of the three models\n",
    " * an assessment of whether there are differences between the models, and if so, which model should be selected according to PSIS-LOO\n",
    " \n",
    "Remember also to comment on the results. Hints and further advice:\n",
    "\n",
    " * In all the three models, use uniform priors for all the parameters to standardize the model assessment results, making the grading easier.\n",
    " * In the hierarchical model, use the same measurement deviation σ for all the machines. This is reasonable, since there are so few measurements per machine, so learning the deviations separately for each machine is difficult. If you want, you can also compute the PSIS-LOO value for the hierarchical model with different σj for each machine and see the effect in the estimated predictive performance (this is not, however, required for full points). For the separate model, you should still use different σj for each machine.\n",
    " * In order to use the psisloo-function, you need to compute the log-likelihood values for each observation and for all the posterior samples. This can be done in the generated quantities block; for a demonstration, see the Gaussian linear model for Kilpisjärvi data in the Matlab Stan examples.\n",
    " * It is reasonable to visualize the k-values for each model, so that you can easily see how many of these values fall in the range 0.5 < k < 1 or k > 1 to assess the reliability of the PSIS-LOO estimate for each of the models.\n",
    " * The estimated effective number of parameters in the model can be computed from equation (7.15) in the book, where lppdloo-cv is the PSIS-LOO value (first output argument of psisloo) and lppd is given by equation (7.5) in the book.\n",
    " * PSIS-LOO is a recently developed method for approxmating the exact LOO and is thus not in BDA3. For more information, see the lecture slides and the original paper by Vehtari, Gelman and Gabry (2015) on arXiv http://arxiv.org/pdf/1507.04544v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11187e890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from psis import psisloo\n",
    "import numpy as np\n",
    "import pystan\n",
    "\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('lines', color='#377eb8', linewidth=2)\n",
    "plt.set_cmap(plt.get_cmap('viridis'));\n",
    "\n",
    "y = np.loadtxt('../ex5/factory.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_b5c8b60b115bf8ed54380ca3ed027dea.\n",
      "20 chains, each with iter=3000; warmup=1500; thin=1; \n",
      "post-warmup draws per chain=1500, total post-warmup draws=30000.\n",
      "\n",
      "              mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "mu[0]        75.39     0.8  24.24  43.01  68.21  76.05  83.51 108.88  916.0   1.02\n",
      "mu[1]       106.22    0.14   9.68  87.45  101.6 106.17 110.86 125.28 4707.0    1.0\n",
      "mu[2]        87.78    0.17  11.01  65.96  82.81   87.8  92.68 108.57 4014.0    1.0\n",
      "mu[3]       111.48    0.09   6.31  99.03 108.48 111.52 114.49 123.57 4514.0    1.0\n",
      "mu[4]        90.06    0.15   9.22  72.14   85.8  90.05  94.43 108.01 3975.0    1.0\n",
      "mu[5]        86.69    0.46  18.72  55.84  78.74  86.17  93.74 119.03 1676.0   1.01\n",
      "sigma[0]     32.31    0.98   31.0  12.88  19.45  25.68  35.77  87.24 1003.0   1.02\n",
      "sigma[1]     18.59    0.19  12.52   7.77  11.58  15.34  21.24  50.26 4519.0    1.0\n",
      "sigma[2]     20.35    0.24  14.64   8.36  12.59  16.62  23.21  55.02 3764.0    1.0\n",
      "sigma[3]     11.86    0.12   7.96   4.95   7.45   9.75  13.57  31.31 4597.0    1.0\n",
      "sigma[4]      17.2    0.19  11.84   7.07  10.63  14.11  19.71  45.84 4017.0    1.0\n",
      "sigma[5]     31.05    0.57  24.99  12.58  18.99  25.14  34.88  85.39 1912.0   1.01\n",
      "log_like[0] -23.16    0.05   1.82 -27.85  -23.8 -22.62 -21.91 -21.47 1606.0   1.01\n",
      "log_like[1] -20.55    0.03   1.64 -25.01 -21.21 -20.05 -19.37 -18.92 4132.0    1.0\n",
      "log_like[2] -20.98    0.03   1.69 -25.55 -21.63 -20.45 -19.76 -19.33 3584.0    1.0\n",
      "log_like[3] -18.32    0.03   1.63 -22.68 -18.98 -17.82 -17.15  -16.7 4233.0    1.0\n",
      "log_like[4] -20.13    0.03   1.68 -24.65  -20.8 -19.62 -18.93 -18.46 3358.0    1.0\n",
      "log_like[5] -23.03    0.03   1.73 -27.63 -23.67 -22.51 -21.81 -21.36 2796.0   1.01\n",
      "lp__        -81.28    0.06   3.25 -88.87 -83.16 -80.83 -78.91 -76.27 3199.0    1.0\n",
      "\n",
      "Samples were drawn using NUTS(diag_e) at Mon Mar 14 17:08:55 2016.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "data_separate = {\n",
    "    'J': y.shape[0],\n",
    "    'K': y.shape[1],\n",
    "    'y': y\n",
    "}\n",
    "code_separate = \"\"\"\n",
    "data {\n",
    "    int<lower=1> J; // number of data points\n",
    "    int<lower=1> K; // number of groups \n",
    "    matrix[J,K]  y; // measurements\n",
    "}\n",
    "parameters {\n",
    "    vector[K] mu; // mean\n",
    "    vector<lower=0>[K] sigma; // std\n",
    "}\n",
    "model {\n",
    "    for (k in 1:K) {\n",
    "      y[:,k] ~ normal(mu[k], sigma[k]);\n",
    "    }\n",
    "}\n",
    "generated quantities {\n",
    "    vector[K] log_like;\n",
    "\n",
    "    for (k in 1:K)\n",
    "      log_like[k] <- normal_log(y[:,k], mu[k], sigma[k]);\n",
    "}\n",
    "\"\"\"\n",
    "fit_separate = pystan.stan(model_code=code_separate, data=data_separate, iter=3000, chains=20)\n",
    "params_separate = fit_separate.extract()\n",
    "print fit_separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-161.923784276\n",
      "[-30.96754925 -25.90462081 -26.55755364 -23.36716085 -25.66227433\n",
      " -29.46462538]\n",
      "[ 1.96311373  1.61775755  1.63485495  1.56473963  1.63354363  1.76972595]\n"
     ]
    }
   ],
   "source": [
    "#print params_separate['log_like'].shape\n",
    "loo, loos, ks = psisloo(params_separate['log_like'])\n",
    "print loo\n",
    "print loos\n",
    "print ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
