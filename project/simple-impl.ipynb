{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13575, 2638)\n",
      "(13575, 1803)\n",
      "(839, 2638)\n",
      "(839, 1803)\n"
     ]
    }
   ],
   "source": [
    "data = Data()\n",
    "train_X, train_S, train_R = data.load(\"0*\", True)\n",
    "print train_X.shape\n",
    "print train_S.shape\n",
    "val_X, val_S, val_R = data.load(\"ff*\")\n",
    "print val_X.shape\n",
    "print val_S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import gammaln\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "class MultinomialBayesianNaiveBayes():\n",
    "    \n",
    "    def __init__(self, class_pseudo_counts=1.0, feature_pseudo_counts=1.0):\n",
    "        self.class_pseudo_counts = class_pseudo_counts\n",
    "        self.feature_pseudo_counts = feature_pseudo_counts\n",
    "        \n",
    "    def betaln(self, x, axis):\n",
    "        return np.sum(gammaln(x), axis=axis) - gammaln(np.sum(x, axis=axis))\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        X: sparse n-samples x n-features\n",
    "        Y: sparse n-samples x n-classes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.W = safe_sparse_dot(Y.T, X) # n-classes x n-features\n",
    "        self.C = Y.sum(axis=0).A         # 1 x n-classes\n",
    "        \n",
    "        self.lprior = np.log(self.C + self.class_pseudo_counts)\n",
    "        \n",
    "    def predict_log_proba(self, x):\n",
    "        \"\"\"\n",
    "        x: sparse n-samples x n-features\n",
    "        \"\"\"\n",
    "        lp = np.zeros((x.shape[0], self.C.size))\n",
    "        for i in range(x.shape[0]):\n",
    "            _,j,xj = find(x[i,:])\n",
    "            wj = self.W[:,j].toarray() + self.feature_pseudo_counts\n",
    "            lp[i,:] = self.lprior + self.betaln(wj + xj, axis=1) - self.betaln(wj, axis=1)\n",
    "\n",
    "        return lp\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        x: sparse n-samples x n-features\n",
    "        \"\"\"\n",
    "        return np.argmax(self.predict_log_proba(x), axis=1)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        x: sparse n-samples x n-features\n",
    "        \"\"\"\n",
    "        lp = self.predict_log_proba(x)\n",
    "        lp -= lp.max(axis=1, keepdims=True)\n",
    "        p = np.exp(lp)\n",
    "        return p/p.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbs = MultinomialBayesianNaiveBayes(class_pseudo_counts = 0, feature_pseudo_counts = 0.01)\n",
    "nbs.fit(train_X, train_S)\n",
    "\n",
    "nbr = MultinomialBayesianNaiveBayes(class_pseudo_counts = 0, feature_pseudo_counts = 0.01)\n",
    "nbr.fit(train_X, train_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_s = np.argmax(val_S.toarray(), axis=1)\n",
    "val_r = np.argmax(val_R.toarray(), axis=1)\n",
    "\n",
    "pred_s = nbs.predict(val_X)\n",
    "pred_r = nbr.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903456495828\n",
      "0.814100610351\n",
      "0.905691019681\n",
      "\n",
      "\n",
      "0.749702026222\n",
      "0.35988271189\n",
      "0.764105435458\n"
     ]
    }
   ],
   "source": [
    "print f1_score(val_s, pred_s, average='micro')\n",
    "print f1_score(val_s, pred_s, average='macro')\n",
    "print f1_score(val_s, pred_s, average='weighted')\n",
    "print \"\\n\"\n",
    "print f1_score(val_r, pred_r, average='micro')\n",
    "print f1_score(val_r, pred_r, average='macro')\n",
    "print f1_score(val_r, pred_r, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
